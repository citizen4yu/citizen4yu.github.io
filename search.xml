<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[qemu block cache]]></title>
    <url>%2F2018%2F10%2F27%2Fqemu-block-cache%2F</url>
    <content type="text"><![CDATA[初始化启动虚拟机后，首先解析cache flagbdrv_parse_cache_mode() cache mode flag writethrough None NOCACHE false Directsync NOCACHE true Writeback 0 false Unsafe NOFLUSH false Writethrough 0 true 设置cache mode变量, drive_new(): no cache.writeback cache.direct cache.no-flush None true true false Directsync false true false Writetback true false false Unsafe true false true Writethrough false false false]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qemu object module]]></title>
    <url>%2F2018%2F09%2F22%2Fqom%2F</url>
    <content type="text"><![CDATA[qom模型中几个重要结构体struct Object struct TypeInfo struct TypeImpl 与TypeInfo基本一样，是全局的设备表，而TypeInfo是在设备驱动中初始化设备信息使用 struct DeviceState 设备的所有配置初始化到该结构体，该结构体可以理解为一个枢纽，与设备object有映射关系，与设备总线也有联系 设备驱动初始化gcc中构造函数特性__attribute__((constructor))，该特性可在main函数之前执行。qemu中所有的设备驱动以构造函数的方式初始化，如下module_init()： /* This should not be used directly. Use block_init etc. instead. */ #define module_init(function, type) static void __attribute__((constructor)) do_qemu_init_ ## function(void) { register_module_init(function, type); } 各设备驱动会通过type_init、block_init、opts_init、trace_init来调用module_init注册驱动。 #define block_init(function) module_init(function, MODULE_INIT_BLOCK) #define opts_init(function) module_init(function, MODULE_INIT_OPTS) #define type_init(function) module_init(function, MODULE_INIT_QOM) #define trace_init(function) module_init(function, MODULE_INIT_TRACE) qemu vl.c中main函数爱执行前会将所有的驱动构造函数执行一遍，来注册各设备驱动，接下来初始化各设备驱动： type_init module_init register_module_init QTAILQ_INSERT_TAIL 注册设备驱动，插入到qom设备链表 main module_call_init e-&gt;init ivshmem_register_types 初始化设备驱动 设备初始化流程（以下以ivshmem设备举例）ivshmem设备qemu cmdline参数：-device ivshmem-doorbell,id=shmem0,chardev=charshmem0,vector=2,ioeventfd=on,master=on,bus=pci.0,addr=0xa -chardev socket,id=charshmem0,path=/tmp/mytest qemu进程启动时，解析命令行，驱动vm都添加了什么设备，再去根据cmdline中设置的参数初始化设备： main qemu_add_opts(&amp;qemu_device_opts) 增加device options list qemu_opts_parse_noisily QEMU_OPTION_device类型会解析各种qemu模拟的设备 qemu_opts_foreach 初始化vm中配置的所有device device_init_func qdev_device_add object_new 创建ivshmem设备object ... ivshmem_doorbell_init 初始化ivshmem设备全局参数 qdev_set_parent_bus 加入到设备总线上 object_property_set_bool ... ivshmem_doorbell_realize 初始化ivshmem设备]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libvirt hashtable原理]]></title>
    <url>%2F2018%2F09%2F15%2Flibvirt-hash%2F</url>
    <content type="text"><![CDATA[libvirt hashablelibvirt hashtable可以理解为一个二维数组，只不过这个数组可以处理不同类型的数据结构，且能高效处理数据（libvirt业务相关的）。 先看2个数据结构：（以libvirt-4.6.0为例） /* * The entire hash table */ struct _virHashTable { virHashEntryPtr *table; uint32_t seed; size_t size; size_t nbElems; virHashDataFree dataFree; virHashKeyCode keyCode; virHashKeyEqual keyEqual; virHashKeyCopy keyCopy; virHashKeyFree keyFree; }; hashtable的设置控制参数、注册回调函数（genkey、freedata等），一维数组。 /* * A single entry in the hash table */ struct _virHashEntry { struct _virHashEntry *next; void *name; void *payload; }; hashtable entry元素，一维数组entry name、 data。 2个重要的函数： uint32_t virHashCodeGen(const void *key, size_t len, uint32_t seed) static int virHashAddOrUpdateEntry(virHashTablePtr table, const void *name, void *userdata, bool is_update) virHashCodeGen() 计算hashtable重要的keyvirHashAddOrUpdateEntry() 向hashtable中插入元素，插入元素的规则是先计算该元素的在hastable中key。找到该key所在链表，再查询是否有冲突的entry name，如果有根据需要是否更新该name的data，或者报错有冲突的name entry。插入key所在链表的方式是头插法。 图说明 缺点当每次更新或插入一个新的entry时，发现所在key值链表大小超过8个时就会将hashtable size扩展： MAX_HASH_LEN * table-&gt;size，MAX_HASH_LEN为8，这样很容易将hashtable设置的很大，而每次扩展都会分配新的内存，有点内存泄露的感觉（不是真的泄露）。]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git proxy方法]]></title>
    <url>%2F2018%2F09%2F09%2Fgit-proxy%2F</url>
    <content type="text"><![CDATA[git代理方法： git config --global core.gitproxy &quot;gitproxy&quot; 其中： gitproxy放于/usr/bin下，内容为： #！/bin/bash # 代理服务器IP PROXY=192.168.0.2 # 代理服务器IP port PROXYPORT=808 exec socat STDIO PROXY:$PROXY:$1:$2,proxyport=$PROXYPORT]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qemu-ga原理]]></title>
    <url>%2F2018%2F09%2F09%2Fqemu-ga%2F</url>
    <content type="text"><![CDATA[libvirt添加channel&lt;channel type=&apos;unix&apos;&gt; &lt;source mode=&apos;bind&apos; path=&apos;/var/lib/libvirt/qemu/vm.agent&apos;/&gt; &lt;target type=&apos;virtio&apos; name=&apos;org.qemu.guest_agent_0&apos; state=&apos;connected&apos;/&gt; &lt;address type=&apos;virtio-serial&apos; controller=&apos;0&apos; bus=&apos;0&apos; port=&apos;1&apos;&gt; &lt;/channel&gt; 构建qemu进程cmdline-chardev socket,id=cahrchannel0,path=/var/lib/libvirt/qemu/vm.agent,server,nowait 字符设备 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 模拟的virtio串口 虚机内部虚机内部会有模拟的串口设备。 /dev/virtio-ports/org.qemu.guest_agent.0 运行qemu-ga程序（qga/main.c） 工作流程qemu将字符设备模拟成virtio串口挂载给虚机，虚机内部通过qemu-ga程序打开模拟的virtio串口设备，并通过该设备实现与libvirtd守护进程的通信，进而实现guest-agent的功能（可查询、设置虚机内部运行态数据）。数据传输流分2部分： （1）libvirtd守护进程与虚机qemu进程间通过“/var/lib/libvirt/qemu/vm.agent” unix-socket通信。 （2）虚机内部与qemu进程间通过模拟的virtio串口通信。]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libvirt threadpool机制]]></title>
    <url>%2F2018%2F09%2F09%2Flibvirt-threadpool%2F</url>
    <content type="text"><![CDATA[1. 初始化一个pthread pool，设置min、max、priority的数量，job回调函数，创建的pthread都是detach的 2. 刚开始只启动min个线程，如果后续job变多，会继续扩展 3. min个线程处于pthread wait状态，等待新的job任务来唤醒 4. 新的job任务来了，获取任务的参数放入job data队列中 5. pthread cond signal唤醒pthread pool中wait态的线程，调用job回调函数处理真正的job处理逻辑]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux module参数设置及查询]]></title>
    <url>%2F2018%2F09%2F09%2Fmodule-set%2F</url>
    <content type="text"><![CDATA[设置： （1）模块未加载 在/etc/modprobe.d/模块名.conf中增加（如果conf文件没有则新建） options 模块名 参数名=参数值 （2）模块已加载 echo 参数值 &gt; /sys/module/模块名/parameters/参数名 查询： cat /sys/module/模块名/parameters/参数名]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coredump]]></title>
    <url>%2F2018%2F09%2F09%2Fcoredump%2F</url>
    <content type="text"><![CDATA[开启应用程序coredump echo &quot;|/usr/share/apport/apport %p %s %c&quot; &gt; /proc/sys/kernel/core_pattern 解析生成的coredump apport-unpack 应用程序的coredump 目标目录 gdb 应用程序路径 Coredump]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[usbredirserver原理]]></title>
    <url>%2F2018%2F09%2F09%2Fusbredirserver%2F</url>
    <content type="text"><![CDATA[基于usbredirserver_0.7.1-1 启动一个usb redirect server，将1个USB设备重定向给usbredirserver /usr/sbin/usbredirserver -p 12345 2-5 将上面重定向的usb server作为redirhost设备，以usb设备的方式挂载给虚拟机libvirt xml格式： &lt;redirdev bus=&apos;usb&apos; type=&apos;tcp&apos;&gt; &lt;source mode=&apos;connect&apos; host=&apos;x.x.x.x&apos; service=&apos;12345&apos;/&gt; &lt;protol tyupe=&apos;raw&apos;&gt; &lt;address type=&apos;usb&apos; bus=&apos;1&apos; port=&apos;1&apos;/&gt; &lt;/redirdev&gt; 在usbredirserver模块中，通过libusb库管理主机上的usb设备，并将usbredirserver通过打开的socket与libusb管理的usb映射，实现usb数据的读写。 main libusb_get_device_list（或libusb_open_device_with_vid_pid，查找给定的usb设备） libusb_open usbrdirhost_open（初始化读写等callback） run_main_loop（真正重定向逻辑，读写逻辑） usbredirhost_read_guest_data（从读usb数据） usbredirhost_write_guest_data（从写usb数据）]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>others</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux daemon创建要点]]></title>
    <url>%2F2018%2F09%2F09%2Fdaemon%2F</url>
    <content type="text"><![CDATA[守护进程创建要点： 改变进程运行目录，一般为root fork子进程，关闭父进程，让init成为新的父进程，并在后台运行 setsid()脱离父进程的会话、进程组 关闭从父进程继承的文件描述符 重定向标准的3个文件描述符（stdout、stdin、stder） 父进程继承的文件掩码mask清零，umask]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux IO多路复用]]></title>
    <url>%2F2018%2F08%2F05%2FIO%2F</url>
    <content type="text"><![CDATA[监听文件描述符的状态来进行相应的读写操作，3个函数： 123selectpollepoll 123456789int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);int poll(struct pollfd *fds, nfds_t nfds, int timeout);int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); select将监听的读、写、except三类文件描述符分开传入，且nfds最大支持1024个。 poll类似于select，只不过将这3类文件描述符放到一个数组中了，同时fds没有最大限制。 select poll需要逐个轮询每个fd的状态，且每个fd的用户空间数据需要独立copy（内核代码： do_sys_poll -&gt; copy_from_user）。 epoll是以上2个函数的增强版，nfds没有最大限制，理论上跟系统支持的最大文件描述符个数相等，所有fd的用户空间数据一次拷贝完成，且不用轮询每个fd的状态，而是通过注册回调的方式，效率明显提高。 各种I/O阻塞时间比较： 参考：Linux I/O详解]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm debug trace]]></title>
    <url>%2F2018%2F07%2F22%2Fkvm-debug-trace%2F</url>
    <content type="text"><![CDATA[1234567891011121. 是能需调试的接口echo 1&gt; /sys/kernel/debug/tracing/events/kvm/kvm_set_irq/enableecho 0禁用trace接口2. 相应操作，调用接口 启动虚机，一定有set irq操作3. 查看接口调用情况cat /sys/kernel/debug/tracing/tracetrace内容清零echo 0 &gt; /sys/kernel/debug/tracing/trace]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm嵌套]]></title>
    <url>%2F2018%2F07%2F22%2Fkvm-nest%2F</url>
    <content type="text"><![CDATA[开启kvm nested特性即可实现kvm嵌套1234561. 在物理服务器上增加配置文件/etc/modprobe.d/kvm_intel.conf，配置如下内容： options kvm_intel nested=12. 关闭所有虚机，重新加载kvm_intel ko modprobe -v -r kvm_intel modprobe -v kvm_intel3. 修改虚机的cpu模式为host-passthrough，则该虚机已实现kvm的嵌套]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libvirt rpc原理]]></title>
    <url>%2F2018%2F06%2F24%2Flibvirt-rpc%2F</url>
    <content type="text"><![CDATA[libvirt rpc原理libvirt一种很流行的虚拟化管理toolkit，基于C语言实现，并且提供python、java、go等多种语言的bindings，常用的命令集工具virsh。bindings和virsh是通过rpc的方式调用不同的虚拟化平台driver的API，达到最终实现功能的效果。其中，libvirt主daemon程序libvirtd作为rpc server端，bindings、virsh作为rpc client。libvirt rpc使用的是标准的XDR协议。libvirt rpc协议。 rpc调用流程： rpc server流程libvirtd main()中，初始化libvirtd server， 再初始化rpc server，并add rpc server到libvirtd server programs链上。当有rpc client端调用时，发送API参数到rpc server，rpc server解析API参数，调用对应的hypervsior driver API，正确处理后返回结果到rpc client。 rpc server端能被多个rpc client连接。 123456789main virNetServerNew virNetServerHandleJob 创建线程池，注册回调 virNetServerProcessMsg 处理rpc server program消息 virNetServerProgramDispatch 分发到remote server program virNetServerProgramDispatchCall 调到remote driver，由remote在调用某一具体hypervisor driver virNetServerProgramNew virNetServerAddProgram virNetDaemonRun mainloop等待rpc client调用 rpc client端流程不管是bindings，还是virsh调用libvirtd hypervisor driver API，首先需要建立connect，这里connect有多种方式（ssh、unix、tls、tcp等），再创建remote client program，组织API参数，发送给rpc server。 rpc client创建的connect可被多个操作使用。 1234567virConnectOpen 或者virConnectOpenAuth、virConnectOpenReadOnly根据需要 virConnectOpenInternal virGetConnect 创建connect object remoteConnectOpen doRemoteOpen virNetClientNewUNIX unix连接方式的client（根据连接方式不同） virNetClientProgramNew 创建remote client program 以list all domain为例，看rpc client调用流程：1234567891011virConnectListAllDomains hypevisor driver API connectListAllDomains remoteConnectListAllDomains src/remote/remote_client_bodies.h,编译时gendispatch.pl根据src/remote/remote_protocol.x自动生成 call callFull virNetClientProgramCall virNetClientSendWithReply virNetClientSendInternal virNetClientIO virNetClientIOEventLoop poll]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qemu memory]]></title>
    <url>%2F2018%2F05%2F17%2Fqemu-memory%2F</url>
    <content type="text"><![CDATA[分为2部分，qemu层初始化，kvm层实际分配。 qemu层初始化注册memory管理callback先上代码栈： main configure_accelerator accel_init_machine kvm_init kvm_memory_listener_register memory_listener_register 重点看以下2个函数和1个结构体：kvm_memory_listener_register()分配system memory listener,最终也是调用的memory_listener_register()memory_listener_register()分配IO memory listenerMemoryListener可以理解为qemu memory管理的函数簇 void kvm_memory_listener_register(KVMState *s, KVMMemoryListener *kml, AddressSpace *as, int as_id) { int i; kml-&gt;slots = g_malloc0(s-&gt;nr_slots * sizeof(KVMSlot)); kml-&gt;as_id = as_id; 分配slot，可配置(libvirt xml memory slot, 或是使用默认32) for (i = 0; i &lt; s-&gt;nr_slots; i++) { kml-&gt;slots[i].slot = i; } 初始化callback kml-&gt;listener.region_add = kvm_region_add; kml-&gt;listener.region_del = kvm_region_del; kml-&gt;listener.log_start = kvm_log_start; kml-&gt;listener.log_stop = kvm_log_stop; kml-&gt;listener.log_sync = kvm_log_sync; kml-&gt;listener.priority = 10; memory_listener_register(&amp;kml-&gt;listener, as); } void memory_listener_register(MemoryListener *listener, AddressSpace *filter) { MemoryListener *other = NULL; AddressSpace *as; listener-&gt;address_space_filter = filter; 如果listener链表为空或是当前listener优先级比链表中最后一个的优先级还大则直接插入链表最后 if (QTAILQ_EMPTY(&amp;memory_listeners) || listener-&gt;priority &gt;= QTAILQ_LAST(&amp;memory_listeners, memory_listeners)-&gt;priority) { QTAILQ_INSERT_TAIL(&amp;memory_listeners, listener, link); } else { listener链表不为空，则找到第1个优先级比其打的插入之前 QTAILQ_FOREACH(other, &amp;memory_listeners, link) { if (listener-&gt;priority &lt; other-&gt;priority) { break; } } QTAILQ_INSERT_BEFORE(other, listener, link); } QTAILQ_FOREACH(as, &amp;address_spaces, address_spaces_link) { listener_add_address_space(listener, as); } } 初始化内存管理数据结构从cpu_exec_init_all()开始，该初始化在MemoryListener之前，初始化IO memory函数簇，IO、system memory的MemoryRegion cpu_exec_init_all io_mem_init memory_region_init_io memory_map_init memory_region_init memory_region_init_io address_space_init memory_region_transaction_commit io_mem_init()初始化IO memory相关的MemoryRegion，最终会调用memory_region_init()，包括初始化必要的函数簇。memory_map_init初始化IO、system memory的MemoryRegion，没有函数簇需要初始化。另外，还是初始化IO、memory重要的全局变量AddressSpace。其中，address_space_init()最重要，初始化了整个qemu进程的memory链。 void address_space_init(AddressSpace *as, MemoryRegion *root, const char *name) { memory_region_ref(root); memory_region_transaction_begin(); as-&gt;ref_count = 1; as-&gt;root = root; 使用上一步初始化的MemoryRegion as-&gt;malloced = false; as-&gt;current_map = g_new(FlatView, 1); flatview_init(as-&gt;current_map); 初始化FlatView as-&gt;ioeventfd_nb = 0; as-&gt;ioeventfds = NULL; QTAILQ_INSERT_TAIL(&amp;address_spaces, as, address_spaces_link); 插入初始化完成的AddressSpace as-&gt;name = g_strdup(name ? name : &quot;anonymous&quot;); address_space_init_dispatch(as); 初始化AddressSpace的dispatch_listener函数簇，个人认为改行放在AddressSpace插入链表前更合理 memory_region_update_pending |= root-&gt;enabled; memory_region_transaction_commit(); } 初始化machine流程中的memorymain machine_class-&gt;init(current_machine) pc_init_isa pc_init1 pc_memory_init 重点是pc_memory_init()，初始化pc.ram（ram-below-4g、ram-above-4g），hotplug-memory（如果支持）、pc firmware、pc.rom、firmware config。 kvm层实际分配内存qemu通过mmap映射完成，到kvm层通过缺页中断实际分配宿主机物理内存。 memory虚拟化性能提升的发展传统的的memory虚拟化流程： GVA -&gt; GPA -&gt; HVA -&gt; HPA 其中GPA -&gt; HVA使用mmap实现 缺点：频繁的VM exit，性能非常低 影子页表： GVA -&gt; HPA 通过CR3寄存器维护影子页表，缩短memory模拟的转换流程 缺点： 每个虚机进程需要维护一个影子页表，如果虚机数量很多，会有非常大的开销 EPT： GVA -&gt; HPA EPT可以理解为硬件方式的影子页表 优点： 解决之前2种方式的缺点]]></content>
      <categories>
        <category>virtual</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github + hexo = blog]]></title>
    <url>%2F2018%2F05%2F13%2Fgithub-hexo-blog%2F</url>
    <content type="text"><![CDATA[github 创建pages1234命名格式：yourname.github.io注意： yourname必须是你的github name，否则blog URL不是以上格式 安装hexo1234567安装hexo-cli npm install -g hexo-clihexo建站 $ hexo init &lt;your_blog_name&gt; $ cd &lt;your_blog_name&gt; $ npm install hexo部署到git上12345678910安装hexo-deployer-git npm install hexo-deployer-git --save修改_config.yml repo: 用git@作前缀，否则每次提交都需要输入github账号密码提交到github $ hexo clean $ hexo g $ hexo d hexo的Next主题12345678910111213下载next主题 在your_blog_name/thems目录下： git clone https://github.com/iissnan/hexo-theme-next themes/next配置为next主题 站点_config.yml下，找到thems字段，修改为next设置favicon图标 准备favicon.ico格式文件，放入source/images目录 修改配置文件_config.yml favicon: small: /images/favicon.ico medium: /images/favicon.ico:w 参考链接主要参考]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>others</tag>
      </tags>
  </entry>
</search>
