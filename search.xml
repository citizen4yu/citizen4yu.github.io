<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[virt_cpu]]></title>
    <url>%2F2018%2F07%2F22%2Fvirt-cpu%2F</url>
    <content type="text"><![CDATA[cpu的虚拟化qemu层vcpu虚拟化qemu层客户机启动时，初始化vcpu，传入CPU id，由kvm真正创建 main machine_run_board_init pc_init_v2_12 pc_init1 pc_cpus_init pc_new_cpu ... (此处省略一堆object操作)、 x86_cpu_realizefn qemu_init_vcpu qemu_kvm_start_vcpu qemu_thread_create(创建vcpu线程) qemu_thread_start qemu_kvm_cpu_thread_fn kvm_init_vcpu kvm_get_vcpu kvm_init_cpu_signals kvm_cpu_exec(运行vcpu) kvm_vcpu_ioctl(KVM_RUN) kvm_arch_post_run 虚拟机启动时根据上层（libvirt或qemu cmdline）配置的vcpu个数、cpu feature、cpu topology，初始化cpu属性（pc_cpus_init-&gt;pc_possible_cpu_arch_ids），在创建vcpu线程（pc_new_cpu）。创建vcpu线程有3个工作要做：第一，初始化vcpu（kvm_init_vcpu中发送IO KVM_CREATE_VCPU创建“kvm-vcpu”文件描述符；通过mmap通过memory；根据不同的hypervisor类型属性初始化kvm_arch_init_vcpu）。第二，初始化vcpu线程signals kvm_init_cpu_signals。第三，vcpu真正的工作逻辑kvm_cpu_exec。 4个重要结构体：1234567typedef struct &#123; uint64_t arch_id; int64_t vcpus_count; CpuInstanceProperties props; Object *cpu; const char *type; &#125; CPUArchId; 初始化vcpu基本配置（cpu type、 arch id、 cpu topolopy）。 123456789101112struct CPUState &#123; /*&lt; private &gt;*/ DeviceState parent_obj; /*&lt; public &gt;*/ int nr_cores; int nr_threads; ... uint16_t pending_tlb_flush; int hvf_fd; &#125;; 这个结构包含vcpu所有信息，主要qemu中cpu运行数据。 12345678910111213141516171819struct X86CPU &#123; /*&lt; private &gt;*/ CPUState parent_obj; /*&lt; public &gt;*/ CPUX86State env; bool hyperv_vapic; bool hyperv_relaxed_timing; int hyperv_spinlock_attempts; char *hyperv_vendor_id; ... int32_t node_id; /* NUMA node this CPU belongs to */ int32_t socket_id; int32_t core_id; int32_t thread_id; int32_t hv_max_vps; &#125;; x86结构下cpu特性使用情况，numa信息，状态（以下CPUX86State）。 1234567891011121314151617181920212223typedef struct CPUX86State &#123; /* standard registers */ target_ulong regs[CPU_NB_REGS]; target_ulong eip; target_ulong eflags; /* eflags register. During CPU emulation, CC flags and DF are set to zero because they are stored elsewhere */ /* emulator internal eflags handling */ target_ulong cc_dst; target_ulong cc_src; target_ulong cc_src2; uint32_t cc_op; ... /* vmstate */ uint16_t fpus_vmstate; uint16_t fptag_vmstate; uint16_t fpregs_format_vmstate; uint64_t xss; TPRAccess tpr_access_type; &#125; CPUX86State; x86 vcpu state，更偏底层，很多寄存器会与linux kernel kvm交互。 kvm层vcpu流程kvm_vm_ioctl (KVM_CREATE_VCPU) kvm_vm_ioctl_create_vcpu kvm_arch_vcpu_create (初始化kvm_vcpu结构) create_vcpu_fd （创建vcpu文件描述符，初始化结构kvm_vcpu_fops） kvm_arch_vcpu_postcreate （设置时钟） kvm中vcpu的创建过程相对简单，3个重要步骤： 第一，初始化kvm_vcpu结构；第二，创建vcpu文件描述符；第三，设置时钟。 vcpu running流程：1234kvm_vcpu_compat_ioctl (由上述create_vcpu_fd初始化kvm_vcpu_fops) kvm_cpu_ioctl (KVM_RUN) kvm_arch_vcpu_ioctl_run _vcpu_run 重要结构体：123456789101112struct kvm_vcpu &#123; struct kvm *kvm; #ifdef CONFIG_PREEMPT_NOTIFIERS struct preempt_notifier preempt_notifier;#endif int cpu; int vcpu_id; int srcu_idx; ... bool preempted; struct kvm_vcpu_arch arch;&#125;; CPU扩展知识点（intel）processor : 3 vendor_id : GenuineIntel 表示cpu供应商，GenuineIntel指intel官方产 cpu family : 6 cpu系列号，可以理解为cpu大版本 1： 表示8086和80186级芯片 2： 表示286级芯片 3： 表示386级芯片 4： 表示486级芯片（SX、 DX、 DX2、 DX4） 5： 表示P5级芯片（Pentium处理器和含MMX技术的Pentium处理器） 6： 表示P6级芯片（包含Celeron、 Pentium 2、 Pentium 3系列） F： 表示Xeon处理器、 Pentium 4处理器 model : 58 表示某系列号下的子系列号，可以理解为cpu的小版本号，同时model还有name，如： Westmere、 SandyBridge model name : Intel(R) Core(TM) i5-3210M CPU @ 2.50GHz stepping : 9 又叫步进值，表示该款产品经历一些迭代版本 microcode : 0x1c 微码版本，可以给cpu打补丁 cpu MHz : 2501.000 cache size : 3072 KB physical id : 6 siblings : 1 core id : 0 cpu cores : 1 apicid : 6 initial apicid : 6 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cpuid_fault fsgsbase tsc_adjust smep arat bugs : bogomips : 5002.00 clflush size : 64 cache_alignment : 64 address sizes : 43 bits physical, 48 bits virtual power management:]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>kvm qemu vcpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm-interrupt]]></title>
    <url>%2F2018%2F07%2F15%2Fkvm-interrupt%2F</url>
    <content type="text"><![CDATA[kvm中断Linux kernel中断分为local APIC和IO API两种中断控制器，local APIC是实际处理中断的组件，IO APIC是分发外部设备的中断请求，最终还是由local APIC来处理中断。另外，多核cpu系统中，没有cpu都有1个ksoftirqd线程专门用于处理CPU上的中断。kvm、qemu的中断是模拟IO APIC组件，实现IO APIC的分发外设中断逻辑。以下基于linux-3.11.1、qemu-2.12.0 1. kvm中断注入准备工作（中断的虚拟化）kvm_vm_ioctl（KVM_IRQ_LINE） kvm_vm_ioctl_irq_line kvm_set_irq（irq_set[i].set） kvm_ioapic_set_irq（有qemu各设备中断初始化中断路由表时设置callback，如下流程） ioapic_service ioapic_deliver kvm_irq_delivery_to_apic kvm_apic_set_irq __apic_accept_irq（APIC_DM_FIXED，kvm_x86_ops-&gt;deliver_posted_interrupt，VMX初始化时设置deliver回调，见如下流程） vmx_deliver_posted_interrupt（设置中断位，并请求事件KVM_REQ_EVENT，等待下次vm entry进行中断注入） qemu进程各设备初始化时注册中断路由表： kvm_irqchip_commit_routes kvm_vm_ioctl（KVM_SET_GSI_ROUTING，调用io进入kvm处理流程） kvm_vm_ioctl（KVM_SET_GSI_ROUTING） kvm_set_irq_routing setup_routing_entry kvm_set_routing_entry kvm_set_pic_irq（传统PCI中断） kvm_set_pic_irq（传统PIC中断） kvm_set_ioapic_irq（MSI中断） VMX初始化： vmx_init kvm_init（vmx_x86_ops, .deliver_posted_interrupt = vmx_deliver_posted_interrupt） 2. kvm中断注入kvm_vm_ioctl（KVM_CREATE_VCPU） kvm_vm_ioctl_create_vcpu create_vcpu_fd（注册kvm_vcpu_fops，.compat_ioctl= kvm_vcpu_compat_ioctl） kvm_vcpu_compat_ioctl（KVM_RUN） kvm_arch_vcpu_ioctl_run __vcpu_run vcpu_enter_guest inject_pending_event（加载到真正的cpu上） kvm_cpu_has_injectable_intr kvm_apic_has_interrupt apic_find_highest_irr（kvm_x86_ops-&gt;sync_pir_to_irr=vmx_sync_pir_to_irr） vmx_sync_pir_to_irr apic_search_irr kvm_queue_interrupt（记录中断到vcpu） vmx_inject_irq（vmx_x86_ops，.set_irq = vmx_inject_irq） vmcs_write32（根据intel硬件手册写到对应寄存器） 3. 中断向量表的初始化qemu中初始化中断向量表： main machine_run_board_init pc_init_v2_12 pc_init1 kvm_pc_setup_irq_routing kvm_irqchip_commit_routes kvm中断向量表的初始化： kvm_vm_ioctl kvm_arch_vm_ioctl kvm_setup_default_irq_routing（default_routing初始化了PIC、IOPIC） kvm_set_irq_routing（分配中断向量表空间，并将default_routing每个中断元素填充到中断向量表的map表中） setup_routing_entry（设置中断entry各元素的值，并插入map表） kvm_set_routing_entry（初始化中断的set callback） kvm_irq_routing_update（更新虚机kvm的中断向量表） 3个重要的结构体： 中断entry信息，x86 32bit共40个，x86 64bit共64个，包括PIC、IOPIC、MSI看后续分析 struct kvm_irq_routing_entry { __u32 gsi; __u32 type; __u32 flags; __u32 pad; union { struct kvm_irq_routing_irqchip irqchip; struct kvm_irq_routing_msi msi; __u32 pad[8]; } u; }; kvm在kernel中的中断entry，其实跟上面的kvm_irq_routing_entry差不多，区别在于增加了set这个callback，上文有说到 struct kvm_kernel_irq_routing_entry { u32 gsi; u32 type; int (*set)(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm, int irq_source_id, int level, bool line_status); union { struct { unsigned irqchip; unsigned pin; } irqchip; struct msi_msg msi; }; struct hlist_node link; }; 真正的kvm中断向量表，包含kvm所有的中断类型PIC、IOPOC、MSI、map用于将同一个gsi号的不同中断进行归类（hash表的味道） struct kvm_irq_routing_table { int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS]; struct kvm_kernel_irq_routing_entry *rt_entries; u32 nr_rt_entries; /* * Array indexed by gsi. Each entry contains list of irq chips * the gsi is connected to. */ struct hlist_head map[0]; }; 4. MSI中断写一段特殊内存产生中断，相比传统PIC中断的好处是不在受中断管脚数量的限制，同时也提高了设备的多个中断的并行效率（因为传统中断因管脚有限，存在复用）。MSI中断实在PCI2.2协议中定义的，其扩展类型MSI-X中断是在PCI3.0协议中定义。]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>kvm qemu interrupt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtio]]></title>
    <url>%2F2018%2F07%2F15%2Fvirtio%2F</url>
    <content type="text"><![CDATA[virtio以下均以网卡virtio代码举例说明（qemu-2.12.0, linux-11.1） 1. 前端驱动前端rececive notify:前端net pci初始化流程： virtio_pci_probe (vp_dev-&gt;vdev.config = &amp;virtio_pci_config_ops) vp_find_vqs vp_try_to_find_vqs setup_vq vring_new_virtqueue (vq-&gt;notify=notify) vp_notify net driver初始化： virtnet_probe try_fill_recv virtqueue_kick virtqueue_notify vq-&gt;notify (此处回调就是上面net pci初始化的vp_notify) vp_notify iowrite16 后端rececive notify:客户机经过net virtio driver的iowrite发送VIRTIO_PCI_QUEUE_NOTIFY后，宿主机就知道有可用的virtio queue，也就是后端，如下： 后端net pci初始化流程： virtio_ioport_write virtio_queue_notify vq-&gt;handle_output（此处回调在qemu virtio net初始化中设置，见如下流程） virtio net初始化： virtio_net_class_init virtio_net_device_realize virtio_net_add_queue virtio_add_queue（vdev-&gt;vq[i].handle_output = handle_output） virtio_net_handle_rx 2. 后端驱动virtio_net_device_realize qemu_new_nic（net_virtio_info结构体中.receive = virtio_net_receive） virtio_net_receive virtio_net_receive_rcu virtqueue_pop（取virtio queue） iov_from_buf（写数据到virtio queue） virtio_notify（发动中断，通知客户机数据写好了） 3. 前后端共享内存的初始化 客户机virtio net驱动初始化时会分配一段内存，在适合时机通知宿主机（write io方式），宿主机得到通知后将客户机获得的这段内存映射到virtio queue virtio_ioport_write（VIRTIO_PCI_QUEUE_PFN） virtio_queue_set_addr virtio_queue_update_rings virtio_init_region_cache]]></content>
      <categories>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>qemu kvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libvirt rpc原理]]></title>
    <url>%2F2018%2F06%2F24%2Flibvirt-rpc%2F</url>
    <content type="text"><![CDATA[libvirt rpc原理libvirt一种很流行的虚拟化管理toolkit，基于C语言实现，并且提供python、java、go等多种语言的bindings，常用的命令集工具virsh。bindings和virsh是通过rpc的方式调用不同的虚拟化平台driver的API，达到最终实现功能的效果。其中，libvirt主daemon程序libvirtd作为rpc server端，bindings、virsh作为rpc client。libvirt rpc使用的是标准的XDR协议。libvirt rpc协议。 rpc调用流程： rpc server流程libvirtd main()中，初始化libvirtd server， 再初始化rpc server，并add rpc server到libvirtd server programs链上。当有rpc client端调用时，发送API参数到rpc server，rpc server解析API参数，调用对应的hypervsior driver API，正确处理后返回结果到rpc client。 rpc server端能被多个rpc client连接。 123456789main virNetServerNew virNetServerHandleJob 创建线程池，注册回调 virNetServerProcessMsg 处理rpc server program消息 virNetServerProgramDispatch 分发到remote server program virNetServerProgramDispatchCall 调到remote driver，由remote在调用某一具体hypervisor driver virNetServerProgramNew virNetServerAddProgram virNetDaemonRun mainloop等待rpc client调用 rpc client端流程不管是bindings，还是virsh调用libvirtd hypervisor driver API，首先需要建立connect，这里connect有多种方式（ssh、unix、tls、tcp等），再创建remote client program，组织API参数，发送给rpc server。 rpc client创建的connect可被多个操作使用。 1234567virConnectOpen 或者virConnectOpenAuth、virConnectOpenReadOnly根据需要 virConnectOpenInternal virGetConnect 创建connect object remoteConnectOpen doRemoteOpen virNetClientNewUNIX unix连接方式的client（根据连接方式不同） virNetClientProgramNew 创建remote client program 以list all domain为例，看rpc client调用流程：1234567891011virConnectListAllDomains hypevisor driver API connectListAllDomains remoteConnectListAllDomains src/remote/remote_client_bodies.h,编译时gendispatch.pl根据src/remote/remote_protocol.x自动生成 call callFull virNetClientProgramCall virNetClientSendWithReply virNetClientSendInternal virNetClientIO virNetClientIOEventLoop poll]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>libvirt rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[qemu memory]]></title>
    <url>%2F2018%2F05%2F17%2Fqemu-memory%2F</url>
    <content type="text"><![CDATA[qemu memory分为2部分，qemu层初始化，kvm层实际分配。 1 qemu层初始化1.1 注册memory管理callback先上代码栈： main configure_accelerator accel_init_machine kvm_init kvm_memory_listener_register memory_listener_register 重点看以下2个函数和1个结构体：kvm_memory_listener_register()分配system memory listener,最终也是调用的memory_listener_register()memory_listener_register()分配IO memory listenerMemoryListener可以理解为qemu memory管理的函数簇 void kvm_memory_listener_register(KVMState *s, KVMMemoryListener *kml, AddressSpace *as, int as_id) { int i; kml-&gt;slots = g_malloc0(s-&gt;nr_slots * sizeof(KVMSlot)); kml-&gt;as_id = as_id; 分配slot，可配置(libvirt xml memory slot, 或是使用默认32) for (i = 0; i &lt; s-&gt;nr_slots; i++) { kml-&gt;slots[i].slot = i; } 初始化callback kml-&gt;listener.region_add = kvm_region_add; kml-&gt;listener.region_del = kvm_region_del; kml-&gt;listener.log_start = kvm_log_start; kml-&gt;listener.log_stop = kvm_log_stop; kml-&gt;listener.log_sync = kvm_log_sync; kml-&gt;listener.priority = 10; memory_listener_register(&amp;kml-&gt;listener, as); } void memory_listener_register(MemoryListener *listener, AddressSpace *filter) { MemoryListener *other = NULL; AddressSpace *as; listener-&gt;address_space_filter = filter; 如果listener链表为空或是当前listener优先级比链表中最后一个的优先级还大则直接插入链表最后 if (QTAILQ_EMPTY(&amp;memory_listeners) || listener-&gt;priority &gt;= QTAILQ_LAST(&amp;memory_listeners, memory_listeners)-&gt;priority) { QTAILQ_INSERT_TAIL(&amp;memory_listeners, listener, link); } else { listener链表不为空，则找到第1个优先级比其打的插入之前 QTAILQ_FOREACH(other, &amp;memory_listeners, link) { if (listener-&gt;priority &lt; other-&gt;priority) { break; } } QTAILQ_INSERT_BEFORE(other, listener, link); } QTAILQ_FOREACH(as, &amp;address_spaces, address_spaces_link) { listener_add_address_space(listener, as); } } 1.2 初始化内存管理数据结构从cpu_exec_init_all()开始，该初始化在MemoryListener之前，初始化IO memory函数簇，IO、system memory的MemoryRegion cpu_exec_init_all io_mem_init memory_region_init_io memory_map_init memory_region_init memory_region_init_io address_space_init memory_region_transaction_commit io_mem_init()初始化IO memory相关的MemoryRegion，最终会调用memory_region_init()，包括初始化必要的函数簇。memory_map_init初始化IO、system memory的MemoryRegion，没有函数簇需要初始化。另外，还是初始化IO、memory重要的全局变量AddressSpace。其中，address_space_init()最重要，初始化了整个qemu进程的memory链。 void address_space_init(AddressSpace *as, MemoryRegion *root, const char *name) { memory_region_ref(root); memory_region_transaction_begin(); as-&gt;ref_count = 1; as-&gt;root = root; 使用上一步初始化的MemoryRegion as-&gt;malloced = false; as-&gt;current_map = g_new(FlatView, 1); flatview_init(as-&gt;current_map); 初始化FlatView as-&gt;ioeventfd_nb = 0; as-&gt;ioeventfds = NULL; QTAILQ_INSERT_TAIL(&amp;address_spaces, as, address_spaces_link); 插入初始化完成的AddressSpace as-&gt;name = g_strdup(name ? name : "anonymous"); address_space_init_dispatch(as); 初始化AddressSpace的dispatch_listener函数簇，个人认为改行放在AddressSpace插入链表前更合理 memory_region_update_pending |= root-&gt;enabled; memory_region_transaction_commit(); } 1.3 初始化machine流程中的memorymain machine_class-&gt;init(current_machine) pc_init_isa pc_init1 pc_memory_init 重点是pc_memory_init()，初始化pc.ram（ram-below-4g、ram-above-4g），hotplug-memory（如果支持）、pc firmware、pc.rom、firmware config。]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>qemu memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github + hexo = blog]]></title>
    <url>%2F2018%2F05%2F13%2Fgithub-hexo-blog%2F</url>
    <content type="text"><![CDATA[github 创建pages1234命名格式：yourname.github.io注意： yourname必须是你的github name，否则blog URL不是以上格式 安装hexo1234567安装hexo-cli npm install -g hexo-clihexo建站 $ hexo init &lt;your_blog_name&gt; $ cd &lt;your_blog_name&gt; $ npm install hexo部署到git上12345678910安装hexo-deployer-git npm install hexo-deployer-git --save修改_config.yml repo: 用git@作前缀，否则每次提交都需要输入github账号密码提交到github $ hexo clean $ hexo g $ hexo d hexo的Next主题12345678910111213下载next主题 在your_blog_name/thems目录下： git clone https://github.com/iissnan/hexo-theme-next themes/next配置为next主题 站点_config.yml下，找到thems字段，修改为next设置favicon图标 准备favicon.ico格式文件，放入source/images目录 修改配置文件_config.yml favicon: small: /images/favicon.ico medium: /images/favicon.ico:w 参考链接主要参考]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>github hexo blog</tag>
      </tags>
  </entry>
</search>
